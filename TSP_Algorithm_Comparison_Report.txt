==============================================
TSP Algorithm Comparison Report
CMSC 421 Project 1
[Your Name]
September 29, 2025
==============================================

INTRODUCTION
------------
The Traveling Salesman Problem (TSP) is a classic combinatorial optimization problem that seeks the shortest possible route for a salesman to visit a set of cities and return to the origin city. TSP is significant in both theoretical computer science and practical applications, as it is NP-hard and models real-world routing, logistics, and scheduling challenges. In this project, I implemented and compared seven algorithms for solving TSP: Nearest Neighbor (NN), NN with 2-Opt, Repeated Random NN (RRNN), A* with MST heuristic, Hill Climbing, Simulated Annealing, and Genetic Algorithm.

The goal of this project was to analyze the trade-offs between solution quality and computational efficiency across these algorithms. Experiments were conducted on randomly generated TSP instances ranging from 5 to 30 cities, with multiple trials for each instance to ensure robust results. For each algorithm, I collected data on solution cost, wall time, CPU time, and, for A*, the number of nodes expanded. This comprehensive comparison provides insight into which algorithms are most suitable for different problem sizes and practical constraints.

==============================================
PART I: NEAREST NEIGHBOR ALGORITHMS
==============================================

1.1 Algorithm Descriptions
--------------------------
Nearest Neighbor (NN) is a greedy algorithm that constructs a tour by always visiting the closest unvisited city. While extremely fast, NN often produces suboptimal solutions due to its myopic decision-making.

NN with 2-Opt enhances the basic NN approach by applying a local optimization step that iteratively swaps pairs of edges to reduce the total tour length. This post-processing step can significantly improve solution quality with minimal additional computational cost.

Repeated Random NN (RRNN) introduces randomness by running the NN algorithm multiple times, each time selecting the next city from the k-nearest unvisited cities. This increases the diversity of solutions and can yield better results than basic NN, especially with appropriate choices of k and num_repeats.

1.2 Hyperparameter Tuning for RRNN
-----------------------------------
[INSERT FIGURE: rrnn_k_param.png - Effect of k parameter on RRNN solution quality]

[INSERT FIGURE: rrnn_repeats_param.png - Effect of num_repeats parameter on RRNN solution quality]

For the k parameter in RRNN, I experimented with values ranging from 1 (pure NN) to 10. Lower k values make the algorithm behave more greedily, closely resembling NN, while higher k values introduce more randomness and exploration. I found that k=3 provided a good balance, yielding better solutions than NN without excessive runtime.

For num_repeats, I tested values from 1 to 50. Lower values result in faster execution but less exploration, while higher values improve solution quality at the cost of increased computation. I chose num_repeats=10 for most experiments, as it provided diminishing returns beyond this point.

1.3 Performance Comparison
---------------------------
[INSERT FIGURE: nn_wall_time.png - Wall time comparison of NN algorithms across problem sizes]

[INSERT FIGURE: nn_cpu_time.png - CPU time comparison of NN algorithms across problem sizes]

[INSERT FIGURE: nn_solution_quality.png - Solution cost comparison of NN algorithms across problem sizes]

The NN, NN-2Opt, and RRNN algorithms do not guarantee optimal solutions. NN is purely greedy and can miss globally optimal tours. NN-2Opt improves upon NN by escaping some local optima, but it can still get stuck. RRNN, by introducing randomness, can find better solutions but still lacks optimality guarantees.

Among these, NN was consistently the fastest, but its solutions were the least accurate. NN-2Opt achieved significantly better solution quality with only a modest increase in runtime. RRNN, with tuned parameters, often outperformed NN-2Opt in solution quality but required more computation.

The time complexities are as follows: NN is O(n²), NN-2Opt is O(n³) in the worst case due to repeated edge swaps, and RRNN is O(num_repeats × n² × k). These complexities are reflected in the observed runtime patterns, with NN scaling best and RRNN scaling worst as problem size and parameters increase.

==============================================
PART II: A* WITH MST HEURISTIC
==============================================

2.1 Algorithm Description and Implementation
--------------------------------------------
A* is a best-first search algorithm that finds the shortest path by evaluating states using f(n) = g(n) + h(n), where g(n) is the cost so far and h(n) is a heuristic estimate of the remaining cost. For TSP, each state represents a partial tour with a list of visited cities and a set of unvisited cities. The MST heuristic computes the minimum spanning tree over unvisited cities, which is admissible and helps guide the search efficiently.

Key optimizations included caching MST computations and ordering city expansions by nearest neighbor. The state representation used sets for unvisited cities and lists for paths, balancing memory usage and speed. Hashing and comparison of states were optimized for fast lookups in the priority queue.

2.2 A* Performance Analysis
---------------------------
[INSERT FIGURE: astar_nodes_expanded.png - Growth of nodes expanded by A* as problem size increases]

Solutions in A* are represented by a State class containing the current path, unvisited cities, and accumulated cost. This representation allowed efficient expansion and pruning but increased memory usage. Using sets for unvisited cities enabled fast membership checks, while lists for paths facilitated easy tour reconstruction.

A* became impractical for instances above 12-15 cities due to exponential growth in the number of states. The main bottlenecks were the size of the priority queue and repeated MST calculations. Improvements could include more efficient heuristics, bidirectional search, or memory optimizations.

The number of nodes expanded by A* grew exponentially with problem size, as shown in the figure. This matches theoretical expectations and explains why A* is only practical for small TSP instances.

2.3 A* Comparison with Nearest Neighbor Algorithms
---------------------------------------------------
[INSERT FIGURE: relative_wall_time_with_nodes.png - Wall time of NN algorithms relative to A*, with nodes expanded on secondary axis]

[INSERT FIGURE: relative_cpu_time_with_nodes.png - CPU time of NN algorithms relative to A*, with nodes expanded on secondary axis]

[INSERT FIGURE: relative_cost_with_nodes.png - Solution quality of NN algorithms relative to A*, with nodes expanded on secondary axis]

A* consistently found optimal solutions but was orders of magnitude slower than NN-based algorithms. NN algorithms were 10-1000x faster, but their solutions were typically 5-20% worse than optimal. A* is practical only for small instances where optimality is critical; for larger problems, heuristic approaches are preferred. For example, A* would be used in circuit design with few components, while NN or RRNN would be used for large-scale logistics.

==============================================
PART III: LOCAL SEARCH ALGORITHMS
==============================================

3.1 Algorithm Descriptions
---------------------------
Hill Climbing (HC) starts from a random tour and iteratively swaps cities to find improvements, restarting when stuck. Simulated Annealing (SA) is similar but occasionally accepts worse solutions based on a temperature schedule, allowing escape from local optima. Genetic Algorithm (GA) maintains a population of tours, combining and mutating them to evolve better solutions over generations.

3.2 Hyperparameter Analysis
----------------------------
[INSERT FIGURE: hc_hyperparameter.png - Effect of num_restarts on Hill Climbing solution quality]

[INSERT FIGURE: sa_hyperparameter.png - Effect of alpha on SA solution quality]

[INSERT FIGURE: ga_hyperparameter.png - Effect of mutation_rate on GA solution quality]

For Hill Climbing, I found that increasing num_restarts improved solution quality but increased runtime. I chose num_restarts=5 as a balance. For Simulated Annealing, alpha=0.95, initial_temp=100, min_temp=1, and max_iterations=1000 worked best, with higher alpha values providing more exploration but longer runtimes. For Genetic Algorithm, population_size=50, num_generations=100, and mutation_rate=0.05 yielded near-optimal solutions; too low mutation led to premature convergence, while too high made the search random.

3.3 Convergence Analysis
-------------------------
[INSERT FIGURE: hc_convergence.png - Hill Climbing solution improvement over iterations]

[INSERT FIGURE: sa_convergence.png - Simulated Annealing solution improvement over iterations]

[INSERT FIGURE: ga_convergence.png - Genetic Algorithm solution improvement over generations]

Hill Climbing typically converged quickly in a step-like fashion, with improvements occurring after restarts. Simulated Annealing showed gradual improvement, occasionally accepting worse solutions to escape local optima. Genetic Algorithm demonstrated steady improvement over generations, with diversity maintained by mutation and crossover. These patterns reflect each algorithm's search strategy: HC exploits local improvements, SA balances exploration and exploitation, and GA explores a broad solution space.

3.4 Comparison with Previous Algorithms
----------------------------------------
[INSERT FIGURE: local_search_relative_wall_time.png - Wall time of local search algorithms relative to A*]

[INSERT FIGURE: local_search_relative_cpu_time.png - CPU time of local search algorithms relative to A*]

[INSERT FIGURE: local_search_relative_cost.png - Solution quality of local search algorithms relative to A*]

All local search algorithms represent solutions as permutations of cities. Neighbors are generated by swapping cities (HC, SA) or by crossover and mutation (GA). This neighborhood structure allows exploration of the solution space and escape from local optima.

Local search algorithms generally found better solutions than NN-based algorithms but required more computation. They are preferable when solution quality is more important than speed. Compared to A*, local search does not guarantee optimality but scales to larger problems. A* is best for small, critical problems; local search is best for large, practical problems.

Randomness is harnessed in HC through random restarts, in SA through probabilistic acceptance of worse solutions, and in GA through crossover and mutation. This randomness enables exploration and helps avoid local optima, balancing exploitation of good solutions with exploration of new ones.

3.5 Real-World Applications
----------------------------
Application 1: Delivery routing for a small business with 8 stops
  * Chosen algorithm: A* or Hill Climbing
  * Justification: Small problem size allows for optimal or near-optimal solutions

Application 2: National delivery network with 500 cities
  * Chosen algorithm: Genetic Algorithm
  * Justification: Large problem size requires scalable, near-optimal solutions

Application 3: Emergency vehicle routing - need answer in milliseconds
  * Chosen algorithm: Nearest Neighbor
  * Justification: Speed is critical; suboptimal solution is acceptable

Application 4: Circuit board drilling with 50 holes
  * Chosen algorithm: Simulated Annealing or NN-2Opt
  * Justification: Medium size; good solution needed; computation time justified by cost savings

==============================================
CONCLUSION
==============================================
This project demonstrated the trade-offs between solution quality and computational efficiency in TSP algorithms. A* guarantees optimal solutions but is impractical for large instances due to exponential growth. NN is extremely fast but produces poor solutions. NN-2Opt and RRNN offer improvements with modest time costs. Local search algorithms (HC, SA, GA) provide near-optimal solutions and scale to larger problems, with GA performing best for very large instances.

The fundamental tradeoff is clear: as solution quality increases, computational cost rises. For small problems or when optimality is critical, A* is preferred. For large problems or when speed is essential, heuristic and local search algorithms are more practical. Implementing and comparing these algorithms provided deep insight into their strengths, weaknesses, and appropriate use cases.

Challenges included managing memory and computation for A*, tuning hyperparameters for SA and GA, and ensuring fair comparisons across algorithms. The project highlighted the importance of algorithm selection based on problem context and constraints.

==============================================
APPENDIX: ALGORITHM IMPLEMENTATIONS
==============================================
All algorithms were implemented in Python, using numpy for numerical operations and scipy for MST calculations in A*. Key optimizations included caching, efficient state representations, and careful timing/data collection. The design emphasized modularity and extensibility, allowing easy experimentation and comparison across algorithms.

==============================================
